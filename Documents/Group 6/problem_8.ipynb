{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90aa16ed",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.generativeai'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m,\u001b[38;5;250m \u001b[39m\u001b[34;01mtime\u001b[39;00m,\u001b[38;5;250m \u001b[39m\u001b[34;01mjson\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m accuracy_score, classification_report\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgoogle\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgenerativeai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgenai\u001b[39;00m \n\u001b[32m      5\u001b[39m API_KEY = \u001b[33m\"\u001b[39m\u001b[33mAIzaSyATWcUQY0YLRuVLyZn7uIhFxBYu05ZI4cI\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      6\u001b[39m genai.configure(api_key=API_KEY)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'google.generativeai'"
     ]
    }
   ],
   "source": [
    "import os, time, json\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import google.generativeai as genai \n",
    "\n",
    "API_KEY = \"AIzaSyATWcUQY0YLRuVLyZn7uIhFxBYu05ZI4cI\"\n",
    "genai.configure(api_key=API_KEY)\n",
    "MODEL_NAME = \"gemini-2.5-flash\"\n",
    "\n",
    "SAMPLE_SIZE = 1000   \n",
    "BATCH_SIZE = 20   \n",
    "\n",
    "print(f\"Testing on {min(SAMPLE_SIZE, len(test_x))} samples...\")\n",
    "sample_texts = test_x[:SAMPLE_SIZE]\n",
    "sample_labels = [label[0] for label in test_y[:SAMPLE_SIZE]]\n",
    "true_labels = [\"Positive\" if label == 1 else \"Negative\" for label in sample_labels]\n",
    "\n",
    "def get_gemini_predictions_batch(texts):\n",
    "    all_predictions = []\n",
    "    \n",
    "    for i in range(0, len(texts), BATCH_SIZE):\n",
    "        batch = texts[i:i+BATCH_SIZE]\n",
    "        \n",
    "        prompt = f\"\"\"Classify each tweet sentiment as \"Positive\" or \"Negative\" only.\n",
    "Return JSON: [{{\"id\":0,\"sentiment\":\"Positive\"}}, {{\"id\":1,\"sentiment\":\"Negative\"}}, ...]\n",
    "\n",
    "Tweets:\n",
    "\"\"\" + \"\\n\".join([f\"{j}. {text}\" for j, text in enumerate(batch)])\n",
    "        \n",
    "        try:\n",
    "            response = genai.GenerativeModel(MODEL_NAME).generate_content(prompt)\n",
    "            results = json.loads(response.text.replace('```json', '').replace('```', '').strip())\n",
    "            \n",
    "            batch_predictions = [\"Positive\"] * len(batch)\n",
    "            for r in results:\n",
    "                if r['id'] < len(batch_predictions):\n",
    "                    batch_predictions[r['id']] = r['sentiment']\n",
    "            all_predictions.extend(batch_predictions)\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Náº¿u fail thÃ¬ Ä‘Ã¡nh nhÃ£n máº·c Ä‘á»‹nh (vd: Negative)\n",
    "            all_predictions.extend([\"Negative\"] * len(batch))\n",
    "    \n",
    "        time.sleep(2)  # nghá»‰ giá»¯a cÃ¡c batch Ä‘á»ƒ trÃ¡nh rate limit\n",
    "    \n",
    "    return all_predictions\n",
    "\n",
    "# Get predictions\n",
    "print(\"Getting Gemini predictions...\")\n",
    "gemini_preds = get_gemini_predictions_batch(sample_texts)\n",
    "\n",
    "# Results\n",
    "gemini_acc = accuracy_score(true_labels, gemini_preds)\n",
    "print(f\"\\nLLM (Gemini) Accuracy: {gemini_acc:.3f} ({gemini_acc*100:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba253604",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Testing on {min(SAMPLE_SIZE, len(test_x))} samples...\")\n",
    "sample_texts = test_x[:SAMPLE_SIZE]\n",
    "sample_labels = [label[0] for label in test_y[:SAMPLE_SIZE]]\n",
    "true_labels = [\"Positive\" if label == 1 else \"Negative\" for label in sample_labels]\n",
    "\n",
    "def get_gemini_predictions_batch(texts):\n",
    "    all_predictions = []\n",
    "    \n",
    "    for i in range(0, len(texts), BATCH_SIZE):\n",
    "        batch = texts[i:i+BATCH_SIZE]\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "You are an expert sentiment classifier. \n",
    "For each tweet below, decide if the sentiment is \"Positive\" or \"Negative\".\n",
    "\n",
    "Guidelines to improve accuracy:\n",
    "- Positive sentiment often includes happiness, excitement, agreement, compliments, or positive emojis like :) ðŸ˜€ â¤ï¸ ðŸ‘.\n",
    "- Negative sentiment often includes anger, sadness, complaints, disagreement, sarcasm, or negative emojis like :( ðŸ˜¡ ðŸ’” ðŸ‘Ž.\n",
    "- Neutral or mixed cases: choose the stronger overall sentiment (Positive or Negative).\n",
    "- Be careful with sarcasm: e.g. \"Yeah right, this is GREAT ðŸ™„\" â†’ Negative.\n",
    "\n",
    "Return the result ONLY in valid JSON array format:\n",
    "[{{\"id\":0,\"sentiment\":\"Positive\"}}, {{\"id\":1,\"sentiment\":\"Negative\"}}, ...]\n",
    "\n",
    "Rules:\n",
    "- No explanations, only the JSON output.\n",
    "- Use exactly \"Positive\" or \"Negative\" (case-sensitive).\n",
    "- Ensure \"id\" matches the tweet index starting from 0.\n",
    "\n",
    "Tweets:\n",
    "\"\"\" + \"\\n\".join([f\"{j}. {text}\" for j, text in enumerate(batch)])\n",
    "\n",
    "        \n",
    "        try:\n",
    "            response = genai.GenerativeModel(MODEL_NAME).generate_content(prompt)\n",
    "            results = json.loads(response.text.replace('```json', '').replace('```', '').strip())\n",
    "            \n",
    "            batch_predictions = [\"Positive\"] * len(batch)\n",
    "            for r in results:\n",
    "                if r['id'] < len(batch_predictions):\n",
    "                    batch_predictions[r['id']] = r['sentiment']\n",
    "            all_predictions.extend(batch_predictions)\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Náº¿u fail thÃ¬ Ä‘Ã¡nh nhÃ£n máº·c Ä‘á»‹nh (vd: Negative)\n",
    "            all_predictions.extend([\"Negative\"] * len(batch))\n",
    "    \n",
    "        time.sleep(2)  # nghá»‰ giá»¯a cÃ¡c batch Ä‘á»ƒ trÃ¡nh rate limit\n",
    "    \n",
    "    return all_predictions\n",
    "\n",
    "# Get predictions\n",
    "print(\"Getting Gemini predictions...\")\n",
    "gemini_preds = get_gemini_predictions_batch(sample_texts)\n",
    "\n",
    "# Results\n",
    "gemini_acc = accuracy_score(true_labels, gemini_preds)\n",
    "print(f\"\\nLLM (Gemini) Accuracy: {gemini_acc:.3f} ({gemini_acc*100:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83f0431",
   "metadata": {},
   "source": [
    "The output results of the Gemini LLM in both cases â€” when allowing the model to process freely and when using a more detailed prompt â€” demonstrate considerable inconsistency and fluctuation. Notably, in many instances, the version with a carefully crafted prompt produced less accurate results compared to when the model was left to infer on its own. This indicates that the modelâ€™s performance is highly dependent on prompt design, and that prompt optimization does not necessarily guarantee better outcomes. Furthermore, in both cases, the LLM delivered lower performance than the machine learning model developed by the team, suggesting that traditional models still hold an advantage in this specific sentiment classification task."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
