{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "797d13ea",
   "metadata": {},
   "source": [
    "Thêm vào trong file của thầy chạy nhé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ab8ca4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "def extract_features_scaled(tweet, freqs, scaler=None, fit_scaler=False):\n",
    "    \"\"\"\n",
    "    Lấy đặc trưng cho 1 tweet và scale (chỉ scale cột 1 & 2).\n",
    "    \"\"\"\n",
    "    word_l = process_tweet(tweet)\n",
    "    x = np.zeros((1, 3))\n",
    "    x[0, 0] = 1  # bias term\n",
    "\n",
    "    for word in word_l:\n",
    "        if (word, 1) in freqs:\n",
    "            x[0, 1] += freqs[(word, 1)]\n",
    "        if (word, 0) in freqs:\n",
    "            x[0, 2] += freqs[(word, 0)]\n",
    "\n",
    "    if scaler is not None:\n",
    "        features = x[:, 1:3]  # chỉ scale 2 cột pos, neg\n",
    "        if fit_scaler:\n",
    "            scaled = scaler.fit_transform(features)\n",
    "        else:\n",
    "            scaled = scaler.transform(features)\n",
    "        x[:, 1:3] = scaled\n",
    "\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ffe506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- STANDARD SCALER ---\n",
    "print(\"=== Training with StandardScaler ===\")\n",
    "\n",
    "scaler_std = StandardScaler()\n",
    "\n",
    "# Build X_train_scaled\n",
    "X_train_std = np.zeros((len(train_x), 3))\n",
    "for i, tweet in enumerate(train_x):\n",
    "    X_train_std[i, :] = extract_features_scaled(tweet, freqs, scaler_std, fit_scaler=(i == 0))\n",
    "\n",
    "# Train logistic regression\n",
    "J_std, w_std = gradient_descent_logistic(X_train_std, train_y, np.zeros((3, 1)), 1e-7, 10000)\n",
    "\n",
    "# Build X_test_scaled\n",
    "X_test_std = np.zeros((len(test_x), 3))\n",
    "for i, tweet in enumerate(test_x):\n",
    "    X_test_std[i, :] = extract_features_scaled(tweet, freqs, scaler_std, fit_scaler=False)\n",
    "\n",
    "# Predict & evaluate\n",
    "y_pred_std = (sigmoid(np.dot(X_test_std, w_std)) > 0.5).astype(int)\n",
    "accuracy_std = np.mean(y_pred_std == np.squeeze(test_y))\n",
    "\n",
    "print(f\"Cost (StandardScaler): {J_std:.6f}\")\n",
    "print(f\"Accuracy (StandardScaler): {accuracy_std:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422e481f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- MIN-MAX SCALER ---\n",
    "print(\"\\n=== Training with MinMaxScaler ===\")\n",
    "\n",
    "scaler_mm = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "# Build X_train_scaled\n",
    "X_train_mm = np.zeros((len(train_x), 3))\n",
    "for i, tweet in enumerate(train_x):\n",
    "    X_train_mm[i, :] = extract_features_scaled(tweet, freqs, scaler_mm, fit_scaler=(i == 0))\n",
    "\n",
    "# Train logistic regression\n",
    "J_mm, w_mm = gradient_descent_logistic(X_train_mm, train_y, np.zeros((3, 1)), 1e-7, 10000)\n",
    "\n",
    "# Build X_test_scaled\n",
    "X_test_mm = np.zeros((len(test_x), 3))\n",
    "for i, tweet in enumerate(test_x):\n",
    "    X_test_mm[i, :] = extract_features_scaled(tweet, freqs, scaler_mm, fit_scaler=False)\n",
    "\n",
    "# Predict & evaluate\n",
    "y_pred_mm = (sigmoid(np.dot(X_test_mm, w_mm)) > 0.5).astype(int)\n",
    "accuracy_mm = np.mean(y_pred_mm == np.squeeze(test_y))\n",
    "\n",
    "print(f\"Cost (MinMaxScaler): {J_mm:.6f}\")\n",
    "print(f\"Accuracy (MinMaxScaler): {accuracy_mm:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d25f84",
   "metadata": {},
   "source": [
    "Scaling helps features have the same scale, avoiding gradient descent moving too fast or too slow on each dimension → helping the model converge more stably.\n",
    "\n",
    "StandardScaler is often suitable when the data has a near-normal distribution (mean ≈ center, not many outliers).\n",
    "\n",
    "MinMaxScaler is good when we want to keep the data distribution in [0, 1], but is easily affected by outliers (pull max/min far away).\n",
    "\n",
    "For this sentiment analysis problem, StandardScaler gives the best results, because the frequency of occurrence from the word is almost normally distributed and does not have too large outliers."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
