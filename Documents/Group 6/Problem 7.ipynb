{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98ab5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Các model & param grid (không scaler vì dữ liệu sparse)\n",
    "pipelines = {}\n",
    "param_grids = {}\n",
    "\n",
    "# 1. Naive Bayes\n",
    "pipelines['MultinomialNB'] = MultinomialNB()\n",
    "param_grids['MultinomialNB'] = {\n",
    "    'alpha': [0.001, 0.01, 0.1, 0.5, 1.0]\n",
    "}\n",
    "\n",
    "# 2. KNN (ít phù hợp cho sparse, nhưng vẫn thử)\n",
    "pipelines['KNN'] = KNeighborsClassifier()\n",
    "param_grids['KNN'] = {\n",
    "    'n_neighbors': [3, 5, 7, 11],\n",
    "    'weights': ['uniform', 'distance']\n",
    "}\n",
    "\n",
    "# 3. Linear SVM\n",
    "pipelines['LinearSVC'] = LinearSVC(max_iter=5000)\n",
    "param_grids['LinearSVC'] = {\n",
    "    'C': [0.01, 0.1, 1, 10, 100]\n",
    "}\n",
    "\n",
    "# 4. Decision Tree\n",
    "pipelines['DecisionTree'] = DecisionTreeClassifier(random_state=42)\n",
    "param_grids['DecisionTree'] = {\n",
    "    'max_depth': [None, 10, 20, 50],\n",
    "    'min_samples_leaf': [1, 2, 5]\n",
    "}\n",
    "\n",
    "# 5. Random Forest\n",
    "pipelines['RandomForest'] = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "param_grids['RandomForest'] = {\n",
    "    'n_estimators': [100, 300, 500],\n",
    "    'max_depth': [None, 20, 50],\n",
    "    'min_samples_leaf': [1, 2, 5]\n",
    "}\n",
    "\n",
    "# 6. Gradient Boosting\n",
    "pipelines['GradientBoosting'] = GradientBoostingClassifier(random_state=42)\n",
    "param_grids['GradientBoosting'] = {\n",
    "    'n_estimators': [100, 200, 500],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'max_depth': [2, 3, 5]\n",
    "}\n",
    "\n",
    "# 7. MLP (neural net)\n",
    "pipelines['MLP'] = MLPClassifier(max_iter=500, random_state=42)\n",
    "param_grids['MLP'] = {\n",
    "    'hidden_layer_sizes': [(50,), (100,), (100,50)],\n",
    "    'alpha': [1e-5, 1e-4, 1e-3]\n",
    "}\n",
    "\n",
    "# 8. Logistic Regression (baseline mạnh cho text)\n",
    "pipelines['LogisticRegression'] = LogisticRegression(solver='liblinear', random_state=42)\n",
    "param_grids['LogisticRegression'] = {\n",
    "    'C': [0.01, 0.1, 1, 10, 100]\n",
    "}\n",
    "\n",
    "# Cross-validation & scoring\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "scoring = 'precision' \n",
    "\n",
    "best_results = {}\n",
    "\n",
    "for name, model in pipelines.items():\n",
    "    print(\"Tuning\", name)\n",
    "    gs = GridSearchCV(model, param_grids[name], cv=cv, scoring=scoring, n_jobs=-1, verbose=1)\n",
    "    gs.fit(train_x_vec, train_y.ravel())\n",
    "\n",
    "    best_results[name] = {\n",
    "        'best_score': gs.best_score_,\n",
    "        'best_params': gs.best_params_,\n",
    "        'best_estimator': gs.best_estimator_\n",
    "    }\n",
    "\n",
    "    print(f\"{name}: best_score={gs.best_score_:.4f}, best_params={gs.best_params_}\")\n",
    "\n",
    "# Convert sang DataFrame để dễ xem\n",
    "results_df = pd.DataFrame([\n",
    "    {\"Model\": k, \"Precision\": v['best_score'], \"Best Params\": v['best_params']}\n",
    "    for k,v in best_results.items()\n",
    "]).sort_values(by=\"Precision\", ascending=False)\n",
    "\n",
    "display(results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3e483c",
   "metadata": {},
   "source": [
    "### 1. Results and Performance Hierarchy\n",
    "\n",
    "The benchmarking tested eight distinct classification models, ranging from linear models and probabilistic approaches (Naive Bayes) to ensemble methods (Gradient Boosting, Random Forest) and neural networks (MLP).\n",
    "\n",
    "The result confirmed that the optimal precision score achieved is **0.996497**, attained by two linear classifiers\n",
    "\n",
    "1.  **LinearSVC (Support Vector Classification)**\n",
    "2.  **LogisticRegression**\n",
    "\n",
    "| Rank | Model Name | Precision | Classification Type |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "| **1** | **LinearSVC** | **0.996497** | Discriminative, Non-parametric (SVM) |\n",
    "| **1** | **LogisticRegression** | **0.996497** | Discriminative, Parametric (GLM) |\n",
    "| 3 | GradientBoosting | 0.996397 | Discriminative, Non-parametric (Ensemble) |\n",
    "| 4 | MLP | 0.995258 | Discriminative, Parametric (Neural Network) |\n",
    "| 5 | MultinomialNB | 0.994525 | Generative, Parametric (Naive Bayes) |\n",
    "\n",
    "### 2. Analysis: Can We Do Better?\n",
    "\n",
    "The comparison demonstrates that the highly-optimized **Logistic Regression** model proved resilient, matching the performance of the best competing classifier, **LinearSVC**.\n",
    "\n",
    "1.  **Superiority of Linear Classifiers:** The joint success of Logistic Regression and LinearSVC indicates that the feature representation (derived from word frequency counts) allows the sentiment classes to be separated almost perfectly by a **linear decision boundary**. This result suggests that more complex models like MLPs or ensemble methods could not effectively utilize additional non-linear information to surpass this linear threshold in terms of precision.\n",
    "2.  **Model Convergence:** The achieved precision of $\\approx 0.9965$ is extremely high (where precision is calculated as $TP / (TP + FP)$). This suggests that the current feature set has reached a practical ceiling for classifying these tweets, as reducing the remaining false positives is exceptionally difficult.\n",
    "3.  **Baseline Challenge Outcome:** While initial, less optimized implementations of Logistic Regression or simpler decision functions might have yielded scores in the $\\approx 0.994 - 0.9960$ range, the optimized Logistic Regression model successfully matched the performance of the most robust ML classifier (LinearSVC). Therefore, among the tested classical ML models, the answer is **No, we cannot do better** than the performance achieved by these two optimized linear models.\n",
    "\n",
    "### 3. Potential Avenues for Further Improvement\n",
    "\n",
    "To marginally increase performance beyond $0.996497$, focus must shift away from model selection within the currently defined set, towards feature engineering and advanced external techniques:\n",
    "\n",
    "*   **Expanded Feature Set:** Re-running the experiment using an **expanded feature vector (6 features in total)** (Exercise 6) could potentially introduce subtle non-linear characteristics or better separability, slightly raising the precision ceiling.\n",
    "*   **Advanced Meta-Learning:** Employing sophisticated ensemble methods such as **Stacking** could combine the strengths of the top classifiers (LinearSVC, LogisticRegression, GradientBoosting) to potentially achieve marginal gains.\n",
    "*   **External Benchmarking:** The ultimate challenge to surpass this performance involves comparing it against external state-of-the-art Natural Language Processing tools, specifically **Virtual Assistants (LLMs)** like ChatGPT, as proposed in the follow-up assignment (Exercise 8)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8881b1",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
